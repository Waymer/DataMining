---
title: "Nick's Write-up"
author: "Nicholas Sclafani"
date: "5/12/2018"
output: pdf_document
latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r}
load("TrainData.Rdata")
library(tidyverse)
```

## Introduction

The data covered in this writeup consists of 248 features, and 133 rows. These features correspond to specific genes, while the rows correspond to patient data. Each of these rows are classified as control or case classes. We aim to explore methods in classifying these patients given their gene information. The various methods we will use include both supervised and unsupervised methods, like hierarchical clustering, k-means, k nearest neighbors, decision trees, regression, etc. We will compare these approaches, as well as their accuracy and misclassification rates. 

## Unsupervised Learning



### PCA

While it is difficult to visualize the structure of 248-dimensional data, we can perform PCA to gather a better sense for the how our data looks along the most important PC directions.

```{r}
x <- data.matrix(trainX)
pc <- prcomp(x)
pc_df <- data.frame(pc$x[,1:10], y = as.factor(trainY))
ggplot(pc_df, aes(x = PC1, y = PC2, color = y)) + geom_point()
plot(cumsum(pc$sdev^2/sum(pc$sdev^2)), main = "Amount of Variance explained by PC directions", xlab = "PC directions", ylab = "Variance")
```

Looking at just the first two principle components we can already some structure in the data with Class 0 points for the most part clustered around the origin and Class 1 points for the most part are negative in PC 1 and around the origin in PC 2.

Plotting the cumulative variance explained by each additional PC direction, a visualization for choosing the number of PC directions can be seen. We can use the first 10 PC direction to explain almost 90 % of the data.

### K-Means

```{r}
# PCA + K-Means
k2 <- kmeans(pc$x[,1:10], centers = 2, iter.max = 100, nstart = 10, 
                algorithm = "Lloyd")
x_df <- data.frame(pc$x[,1:10], cluster1 = as.factor((k2$cluster - 2)*-1),
                   cluster2 = as.factor(k2$cluster - 1))
x_df$y <- as.factor(trainY)

if (sum(x_df$cluster1 != trainY) < sum(x_df$cluster2 != trainY)) {
  x_df$cluster = x_df$cluster1
} else {
  x_df$cluster = x_df$cluster2
}

error_rate = sum(x_df$cluster != trainY) / 133

x_df <- x_df %>% mutate(correct = 
                          case_when(
                            (cluster !=  y) ~ "x",
                            TRUE ~ ""))
ggplot(x_df, aes(x = PC1, y = PC2)) + geom_point(aes(color = cluster)) + 
  geom_text(aes(label = correct))
```

We can see even with the just the first 2 PC scores that the mistakes were all made at points that should be in Class 1 but that lie near the origin where most of Class 0 is clustered. The error rate is:
```{r}
error_rate
```

We noticed that the error rate without dimension reduction is about the same, but with more severe dimension reduction, i.e. using only the first two principal components, there is a slight increase in error.

### Hierarchical Clustering

```{r}
#Average Linkage
arcluster <- hclust(dist(data.frame(x_df)), method = "average")
plot(arcluster, main = "Non-PCA Average Hierarchical Clustering", xlab = "")
arclusterCut <- cutree(arcluster, 2)
arclusterCut[arclusterCut == 1] = 0; arclusterCut[arclusterCut == 2] = 1
print("Non-PCA Average Hierarchical Classifcation Error: "); sum(arclusterCut != trainY) / 133

acluster <- hclust(dist(data.frame(trainX)), method = "average")
plot(acluster, main = "Average Hierarchical Clustering", xlab = "")
aclusterCut <- cutree(acluster, 2)
aclusterCut[aclusterCut == 1] = 0; aclusterCut[aclusterCut == 2] = 1
print("Average Hierarchical Classifcation Error: "); sum(aclusterCut != trainY) / 133

#Single Linkage
srcluster <- hclust(dist(data.frame(x_df)), method = "single")
srclusterCut <- cutree(srcluster, 2)
srclusterCut[srclusterCut == 1] = 0; srclusterCut[srclusterCut == 2] = 1
print("Non-PCA Single Hierarchical Classifcation Error: "); sum(srclusterCut != trainY) / 133

scluster <- hclust(dist(data.frame(trainX)), method = "single")
sclusterCut <- cutree(scluster, 2)
sclusterCut[sclusterCut == 1] = 0; sclusterCut[sclusterCut == 2] = 1
print("Single Hierarchical Classifcation Error: "); sum(sclusterCut != trainY) / 133

#Complete Linkage
crcluster <- hclust(dist(data.frame(x_df)), method = "complete")
crclusterCut <- cutree(crcluster, 2)
crclusterCut[crclusterCut == 1] = 0; crclusterCut[crclusterCut == 2] = 1
print("Non-PCA Complete Hierarchical Classifcation Error: "); sum(crclusterCut != trainY) / 133

ccluster <- hclust(dist(data.frame(trainX)), method = "complete")
cclusterCut <- cutree(ccluster, 2)
cclusterCut[cclusterCut == 1] = 0; cclusterCut[cclusterCut == 2] = 1
print("Complete Hierarchical Classifcation Error: "); sum(cclusterCut != trainY) / 133
```

We can see that each method, Complete, Single, and Average all yield rather similar results. While the hierarchical clustering results for Complete and Single are not shown, they are visually extremely close to that of average hierarchical clustering. The branches have poor distribution, so cutting at small levels like 3 would give wildly inconsistent groupings, with one group with few elements and another with many. 

However, this does not necessarily mean this clustering method would be inappropriate for this context. Taking a closer look at the errors, we see that the classification errors are actually somewhat low. Average linkage has the lowest error, while single has the highest error. With an error of ~12.8%, average hierarchical clustering is a viable method for this problem. 

Additionally, reducing the dimensions through PCA does not seem to have much affect on the clustering reuslts. The classification errors for both average and single linkage clustering stayed exactly the same with or without prior PCA. However complete linakge seemed to change, although very minor. While dimension reduction was not too useful in this case, dimension reduction is still important for ease of use, visualization, and overall handling. 

## Supervised Learning

## Logistic Regression

Logistic regression fits a model to predict the outcome of a binary response variable. This is appropriate in this case because our response variable is binary and takes on values of either 0 or 1. In order to fit a logistic regression to this data, first we reduced the dimension of the data through PCA, and then we fit a logistic regression to predict the response variables using the first 2 principal component scores as predictors. The first 2 principal component scores are the ones that explain most of the variability in the data, and reducing to 2 dimensions allows us to create more interpretable visualizations.

Using a logistic regression with the first 2 PCA scores as predictors resulted in a test error of 5.9%, using 5-fold cross-validation. 

We also considered fitting nonlinear, more complex logistic regression models instead of the simple linear regression on two PC scores. However, the more complex fits did not result in any improvement on the errors, and in terms of visual interpretation the decision boundary did not appear to change. This indicated that the simple linear fit was more appropriate than a quadratic or a cubic fit. The decision boundary for this is shown below.

```{r}
Xpc = prcomp(trainX, center = FALSE)
first2scores = Xpc$x[,1:2]

lreg = glm(formula = trainY ~ first2scores, family = "binomial")

plot(first2scores, pch = as.character(trainY))
abline(a = lreg$coefficients[3]/lreg$coefficients[1], b = -lreg$coefficients[2]/lreg$coefficients[1], col = "blue", lwd = 2)
```

### K-NN

For K-NN, our goal is to determine the number of neighbors, K, that best classifies our data. In order to do this, we ran 5-fold cross-validaiton. The smallest error was consistentily found with K = 1, K = 2, K = 3, and K = 4. While our first instict was to choose K = 3 to prevent noise, we found that K = 1 consistently had the smallest standard deviation in its error results, and this a 1 Nearest Neighbor model is our optimal choice for a K-NN model.
```{r}
set.seed(99)
library(class)
# without dimension reduction
folds <- sample(rep(1:5, 27), replace = FALSE)[1:133]
misclass <- matrix(rep(-1, 100), ncol = 5)
for (m in 1:20) {
  for (f in 1:5) {
    xtr <- trainX[which(folds != f),]
    xte <- trainX[which(folds == f),] 
    ytr <- trainY[which(folds != f)]
    yte <- trainY[which(folds == f)]
    
    yhat <- knn(xtr, xte, ytr, k = m)
    misclass[m, f] = sum(yhat != yte) / length(yhat)
  }
}
c <- apply(misclass, 1, mean)
sd <- apply(misclass, 1, sd)
knn_df <- data.frame(c, sd, k = 1:20)
ggplot(knn_df, aes(x = k, y = c)) + geom_point() + 
  geom_errorbar(aes(ymin=c-sd, ymax=c+sd)) + 
  labs(x = "K", y = "Misclassification Rate")
```

###Classification Trees

For our classification tree, we ran 5-fold cross validation to create a new tree for each different training set and compared the test errors. We found that the tree created from this method of cross validation was the same as the original tree created by using the entire training data set. We also used cross validation to determine the optimal tree size and found that it was actually a classification stump, with only one split in the tree on the value of the variable X206. 

```{r, message=F,warning=F}
library(tree)
x <- read.table(file="trainX.txt",header=TRUE)
y <- read.table(file="trainy.txt",header=TRUE)
y$label = factor(y$label)
train = cbind(x,y)
tree.oj = tree(label~., data = train)
prune.oj <- prune.misclass(tree.oj, best = 2)
plot(prune.oj)
text(prune.oj, pretty = 0)
```

Trees have inherently high variance and so if the values for X206 correspond to different labels in the test data, then the tree would produce a larger misclassification rate. Therefore, we decided to look at random forests, boosting, and bagging. Because we only had one error for our decision tree on the training set, we feel that boosting would not have a significant effect on classification. We decided to use random forests and bagging in order to reduce the variance in order to achieve better results on the test data. We found that bagging and random forests achieved the same misclassification rate on the training data (3%), which was slightly higher than the misclassification rate for the single decision tree (0.75%).
