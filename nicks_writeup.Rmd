---
title: "Nick's Write-up"
author: "Nicholas Sclafani"
date: "5/12/2018"
output: pdf_document
latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r}
load("TrainData.Rdata")
library(tidyverse)
```

## Introduction

The data covered in this writeup consists of 248 features, and 133 rows. These features correspond to specific genes, while the rows correspond to patient data. Each of these rows are classified as control or case classes. We aim to explore methods in classifying these patients given their gene information. The various methods we will use include both supervised and unsupervised methods, like hierarchical clustering, k-means, k nearest neighbors, decision trees, regression, etc. We will compare these approaches, as well as their accuracy and misclassification rates. 

## Unsupervised Learning



### PCA

While it is difficult to visualize the structure of 248-dimensional data, we can perform PCA to gather a better sense for the how our data looks along the most important PC directions.

```{r}
x <- data.matrix(trainX)
pc <- prcomp(x)
pc_df <- data.frame(pc$x[,1:10], y = as.factor(trainY))
ggplot(pc_df, aes(x = PC1, y = PC2, color = y)) + geom_point()
plot(cumsum(pc$sdev^2/sum(pc$sdev^2)), main = "Amount of Variance explained by PC directions", xlab = "PC directions", ylab = "Variance")
```

Looking at just the first two principle components we can already some structure in the data with Class 0 points for the most part clustered around the origin and Class 1 points for the most part are negative in PC 1 and around the origin in PC 2.

Plotting the cumulative variance explained by each additional PC direction, a visualization for choosing the number of PC directions can be seen. We can use the first 10 PC direction to explain almost 90 % of the data.

### K-Means

```{r}
# PCA + K-Means
k2 <- kmeans(pc$x[,1:10], centers = 2, iter.max = 100, nstart = 10, 
                algorithm = "Lloyd")
x_df <- data.frame(pc$x[,1:10], cluster1 = as.factor((k2$cluster - 2)*-1),
                   cluster2 = as.factor(k2$cluster - 1))
x_df$y <- as.factor(trainY)

if (sum(x_df$cluster1 != trainY) < sum(x_df$cluster2 != trainY)) {
  x_df$cluster = x_df$cluster1
} else {
  x_df$cluster = x_df$cluster2
}

error_rate = sum(x_df$cluster != trainY) / 133

x_df <- x_df %>% mutate(correct = 
                          case_when(
                            (cluster !=  y) ~ "x",
                            TRUE ~ ""))
ggplot(x_df, aes(x = PC1, y = PC2)) + geom_point(aes(color = cluster)) + 
  geom_text(aes(label = correct))
```

We can see even with the just the first 2 PC scores that the mistakes were all made at points that should be in Class 1 but that lie near the origin where most of Class 0 is clustered. The error rate is:
```{r}
error_rate
```

We noticed that the error rate without dimension reduction is about the same, but with more severe dimension reduction, i.e. using only the first two principal components, there is a slight increase in error.

### Hierarchical Clustering

```{r}
#Average Linkage
arcluster <- hclust(dist(data.frame(x_df)), method = "average")
plot(arcluster, main = "Non-PCA Average Hierarchical Clustering", xlab = "")
arclusterCut <- cutree(arcluster, 2)
arclusterCut[arclusterCut == 1] = 0; arclusterCut[arclusterCut == 2] = 1
print("Non-PCA Average Hierarchical Classifcation Error: "); sum(arclusterCut != trainY) / 133

acluster <- hclust(dist(data.frame(trainX)), method = "average")
plot(acluster, main = "Average Hierarchical Clustering", xlab = "")
aclusterCut <- cutree(acluster, 2)
aclusterCut[aclusterCut == 1] = 0; aclusterCut[aclusterCut == 2] = 1
print("Average Hierarchical Classifcation Error: "); sum(aclusterCut != trainY) / 133

#Single Linkage
srcluster <- hclust(dist(data.frame(x_df)), method = "single")
srclusterCut <- cutree(srcluster, 2)
srclusterCut[srclusterCut == 1] = 0; srclusterCut[srclusterCut == 2] = 1
print("Non-PCA Single Hierarchical Classifcation Error: "); sum(srclusterCut != trainY) / 133

scluster <- hclust(dist(data.frame(trainX)), method = "single")
sclusterCut <- cutree(scluster, 2)
sclusterCut[sclusterCut == 1] = 0; sclusterCut[sclusterCut == 2] = 1
print("Single Hierarchical Classifcation Error: "); sum(sclusterCut != trainY) / 133

#Complete Linkage
crcluster <- hclust(dist(data.frame(x_df)), method = "complete")
crclusterCut <- cutree(crcluster, 2)
crclusterCut[crclusterCut == 1] = 0; crclusterCut[crclusterCut == 2] = 1
print("Non-PCA Complete Hierarchical Classifcation Error: "); sum(crclusterCut != trainY) / 133

ccluster <- hclust(dist(data.frame(trainX)), method = "complete")
cclusterCut <- cutree(ccluster, 2)
cclusterCut[cclusterCut == 1] = 0; cclusterCut[cclusterCut == 2] = 1
print("Complete Hierarchical Classifcation Error: "); sum(cclusterCut != trainY) / 133
```

We can see that each method, Complete, Single, and Average all yield rather similar results. While the hierarchical clustering results for Complete and Single are not shown, they are visually extremely close to that of average hierarchical clustering. The branches have poor distribution, so cutting at small levels like 3 would give wildly inconsistent groupings, with one group with few elements and another with many. 

However, this does not necessarily mean this clustering method would be inappropriate for this context. Taking a closer look at the errors, we see that the classification errors are actually somewhat low. Average linkage has the lowest error, while single has the highest error. With an error of ~12.8%, average hierarchical clustering is a viable method for this problem. 

Additionally, reducing the dimensions through PCA does not seem to have much affect on the clustering reuslts. The classification errors for both average and single linkage clustering stayed exactly the same with or without prior PCA. However complete linakge seemed to change, although very minor. While dimension reduction was not too useful in this case, dimension reduction is still important for ease of use, visualization, and overall handling. 

## Supervised Learning

### K-NN

For K-NN, our goal is to determine the number of neighbors, K, that best classifies our data. In order to do this, we ran 5-fold cross-validaiton. The smallest error was consistentily found with K = 1, K = 2, K = 3, and K = 4. While our first instict was to choose K = 3 to prevent noise, we found that K = 1 consistently had the smallest standard deviation in its error results, and this a 1 Nearest Neighbor model is our optimal choice for a K-NN model.
```{r}
set.seed(99)
library(class)
# without dimension reduction
folds <- sample(rep(1:5, 27), replace = FALSE)[1:133]
misclass <- matrix(rep(-1, 100), ncol = 5)
for (m in 1:20) {
  for (f in 1:5) {
    xtr <- trainX[which(folds != f),]
    xte <- trainX[which(folds == f),] 
    ytr <- trainY[which(folds != f)]
    yte <- trainY[which(folds == f)]
    
    yhat <- knn(xtr, xte, ytr, k = m)
    misclass[m, f] = sum(yhat != yte) / length(yhat)
  }
}
c <- apply(misclass, 1, mean)
sd <- apply(misclass, 1, sd)
knn_df <- data.frame(c, sd, k = 1:20)
ggplot(knn_df, aes(x = k, y = c)) + geom_point() + 
  geom_errorbar(aes(ymin=c-sd, ymax=c+sd)) + 
  labs(x = "K", y = "Misclassification Rate")
```
